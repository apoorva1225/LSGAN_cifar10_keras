{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSGAN_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-o6hPMpD5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Reshape\n",
        "\n",
        "#discriminator model\n",
        "def discriminator(in_shape=(32,32,3)):\n",
        "  model = Sequential()\n",
        "\n",
        "  #normal\n",
        "  model.add(Conv2D(64, (3,3), padding='same', input_shape=in_shape))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  #downsampling with 2x2 stride \n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  #downsampling with 2x2 stride \n",
        "  model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  #downsampling with 2x2 stride \n",
        "  model.add(Conv2D(256, (3,3), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  #classifier\n",
        "  model.add(Flatten())\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Dense(1, activation='linear'))\n",
        "  \n",
        "  #compile model\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile( loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n",
        "  \n",
        "  return model\n",
        "\n",
        "#generator model\n",
        "def generator(latent_dim):\n",
        "  model = Sequential()\n",
        "  \n",
        "  #image foundation 4x4\n",
        "  n_node = 256*4*4\n",
        "  model.add(Dense(n_node, input_dim=latent_dim))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  model.add(Reshape((4,4, 256)))\n",
        "  \n",
        "  #upsampling with 2x2 stride \n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  #upsampling with 2x2 stride \n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "\n",
        "  #upsampling with 2x2 stride \n",
        "  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.2))\n",
        "  \n",
        "  #output layer\n",
        "  model.add(Conv2D(3, (3,3), activation='tanh', padding='same'))\n",
        "\n",
        "  return model\n",
        "\n",
        "#combined generator and discriminator model\n",
        "def gan(g_model, d_model):\n",
        "  #making discriminator model not trainable, while training generator\n",
        "  d_model.trainable = False\n",
        "  \n",
        "  model = Sequential()\n",
        "\n",
        "  #add generator model\n",
        "  model.add(g_model)\n",
        "\n",
        "  #add discriminator model\n",
        "  model.add(d_model)\n",
        "  \n",
        "  #compile them\n",
        "  opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "  model.compile(loss='mean_squared_error', optimizer=opt)\n",
        "  \n",
        "  return model\n",
        "\n",
        "#load images from cifar10 dataset\n",
        "def load_real_samples():\n",
        "  #load dataset\n",
        "  (x_train, _),(_,_) = load_data()\n",
        "  #convert unsigned int to float\n",
        "  x = x_train.astype('float32')\n",
        "  #scale from [0,255] to [-1,1]\n",
        "  x = (x-127.5)/127.5\n",
        "  return x\n",
        "\n",
        "#select real images\n",
        "def generate_real_samples(dataset, n_samples):\n",
        "  #choose x random instances btw [0, no_of_records_in_dataset] where x = n_samples\n",
        "  rn = np.random.randint(0, dataset.shape[0], n_samples)\n",
        "  #retrieve selected images\n",
        "  x = dataset[rn]\n",
        "  #create 'real' class labels (1)\n",
        "  y = np.ones((n_samples,1))\n",
        "  return x,y\n",
        "\n",
        "#generate points in latent space as input for generator\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "  #generate points\n",
        "  x_input = np.random.randn(latent_dim*n_samples)\n",
        "  #reshape them into a batch of inputs\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\n",
        "  return x_input\n",
        "\n",
        "#generate fake images using generator\n",
        "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
        "  #generate points in latent space (random noise)\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\n",
        "  #feed these points (noise) to generator for fake image output\n",
        "  x = g_model.predict(x_input)\n",
        "  #create 'fake' class labels (0)\n",
        "  y = np.zeros((n_samples,1))\n",
        "  return x,y\n",
        "\n",
        "#display fake images generated by generator\n",
        "def save_plot(examples, epoch, n = 7):\n",
        "  examples = (examples + 1)/2.0\n",
        "  \n",
        "  for i in range(n*n):\n",
        "    pyplot.subplot(n, n, i+1)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(examples[i])\n",
        "    \n",
        "  filename = \"generated_plot_e{epoch+1}.png\"\n",
        "  pyplot.savefig(filename)\n",
        "  pyplot.show()\n",
        "  pyplot.close()\n",
        "\n",
        "#evaluate discriminator and display fake images generated\n",
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\n",
        "  #real images samples\n",
        "  x_real, y_real = generate_real_samples(dataset, n_samples)\n",
        "  #evaluate discriminator on real images\n",
        "  _, accuracy_real = d_model.evaluate(x_real, y_real, verbose=0)\n",
        "  \n",
        "  #fake image samples generated by generator\n",
        "  x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "  #evaluate discriminator on fake images\n",
        "  _, accuracy_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
        "\n",
        "  #summarize discriminator performance by accuracy\n",
        "  print(f\"Accuracy on real={accuracy_real*100}% & on fake={accuracy_fake*100}%\")\n",
        "\n",
        "  #display fake images generated by generator\n",
        "  save_plot(x_fake, epoch)\n",
        "  #save generator model\n",
        "  filename = f\"generator_model_{epoch+1}.h5\"\n",
        "  g_model.save(filename)\n",
        "\n",
        "#plot loss in each iteration\n",
        "def plot_history(d1_hist, d2_hist, g_hist):\n",
        "  pyplot.plot(d1_hist, label='D_1oss1')\n",
        "  pyplot.plot(d2_hist, label='D_1oss2')\n",
        "  pyplot.plot(g_hist, label='G_1oss')\n",
        "  pyplot.legend()\n",
        "  filename = \"loss_line_plot.png\"\n",
        "  pyplot.savefig(filename)\n",
        "  pyplot.show()\n",
        "  pyplot.close()\n",
        "\n",
        "#train generator and discriminator\n",
        "def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=50, n_batch=128):\n",
        "  batch_per_epoch = int(dataset.shape[0]/n_batch)\n",
        "  half_batch = int(n_batch/2)\n",
        "  #lists for appending loss for each iteration\n",
        "  d1_hist, d2_hist, g_hist = list(), list(), list()\n",
        "\n",
        "  for i in range(n_epochs):\n",
        "   \n",
        "    for j in range(batch_per_epoch):\n",
        "      #randomly selected real images\n",
        "      x_real, y_real = generate_real_samples(dataset, half_batch)\n",
        "      #training discriminator model for above real samples\n",
        "      d_loss1,_ = d_model.train_on_batch(x_real, y_real)\n",
        "      \n",
        "      #generating fake samples\n",
        "      x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
        "      #training discriminator model for above fake samples\n",
        "      d_loss2,_ = d_model.train_on_batch(x_fake, y_fake)\n",
        "      \n",
        "      #generating points in latent space as input to generator\n",
        "      x_gan = generate_latent_points(latent_dim, n_batch)\n",
        "      #creating inverted labels for fake images\n",
        "      y_gan =np.ones((n_batch,1))\n",
        "      #training the generator via discriminator's errors\n",
        "      g_loss = gan_model.train_on_batch(x_gan, y_gan)\n",
        "\n",
        "      #summarizing losses in this batch\n",
        "      print(f\"{i+1}--{j+1}/{batch_per_epoch}-- D1={d_loss1}, D2={d_loss2}, G={g_loss}\")\n",
        "      \n",
        "      #appending above losses to loss list for plotting at the end\n",
        "      d1_hist.append(d_loss1)\n",
        "      d2_hist.append(d_loss2)\n",
        "      g_hist.append(g_loss)\n",
        "\n",
        "    #evaluating the model performance at intervals\n",
        "    if (i+1)%10 == 0:\n",
        "      summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
        "\n",
        "  #plotting the losses recorded during complete training    \n",
        "  plot_history(d1_hist, d2_hist, g_hist)\n",
        "\n",
        "#latent space dimension\n",
        "latent_dim = 100\n",
        "\n",
        "#create discriminator\n",
        "d_model = discriminator()\n",
        "\n",
        "#create generator\n",
        "g_model = generator(latent_dim)\n",
        "\n",
        "#create gan\n",
        "lsgan_model = gan(g_model, d_model)\n",
        "\n",
        "#load real image data\n",
        "dataset = load_real_samples()\n",
        "\n",
        "#train model\n",
        "train(g_model, d_model, lsgan_model, dataset, latent_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}